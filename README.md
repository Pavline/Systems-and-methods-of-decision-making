# Systems-and-methods-of-decision-making
# Тема: «Метрические	алгоритмы	классификации»

Метрические алгоритмы основываются на гипотезе компактоности, в которой сказано, что схожим объектам соответствуют схожие ответы. 

Для формализации понятия «сходства» вводится функция расстояния в пространстве объектов X.

ρ: X×X→ℝ ; ρ (X, X') - это мера близости, которая показывает насколько X похож на X'.

Метрические методы - это методы, которые основаны на анализе сходства объекстов.

Метрический алгоритм классификации  относит объект u к тому классу y , для которого суммарный вес ближайших обучающих объектов максимален.


К метрическим методам классификации относятся:
- KNN - алгоритм k ближайших соседей
- KwNN - алгоритм k взвешенных ближайших соседей
- PW - метод парзеновского окна
- PF - метод	потенциальных	функций
- STOLP - алгоритм отбора	эталонных	объектов


Для поиска оптимальных параметров для каждого из рассматриваемых ниже метрических алгоритмов используется критерий скользящего контроля LOO (leave-one-out), который состоит в следующем:
1.
2
3
4
При минимальном значении LOO получим оптимальный параметр алгоритма.


## Алгоритм	ближайших	соседей
### Алгоритм 	k ближайших	соседей	- KNN
Алгоритм анализа k ближайших соседей kNN относит классифицируемый объект к тому классу, элементов которого больше среди k ближайших соседей.

![Иллюстрация к проекту](https://github.com/Pavline/Systems-and-methods-of-decision-making/blob/master/knn_tr.png)

При k = 1 этот алгоритм неустойчив к шуму. При k = ℓ, наоборот, он чрезмерно устойчив и вырождается в константу. Таким образом, крайние значения k нежелательны. На практике оптимальное значение параметра k определяют по критерию скользящего контроля с исключением объектов по одному (leave-one-out, LOO).

Стоит заметить, что если классифицируемый объект xi не исключать из обучающей выборки, то ближайшим соседом xi всегда будет сам xi, и минимальное (нулевое) значение функционала LOO(k) будет достигаться при k = 1.

### Алгоритм 	k взвешенных ближайших	соседей	- KwNN

#### Недостатки простейших метрических алгоритмов типа kNN.
- Приходится хранить обучающую выборку целиком. Это приводит к неэффективному расходу памяти и чрезмерному усложнению решающего правила.
- Поиск ближайшего соседа предполагает сравнение классифицируемого объекта со всеми объектами выборки за O(ℓ) операций. Для задач с большими выборками или высокой частотой запросов это может оказаться накладно. Проблема решается с помощью эффективных алгоритмов поиска ближайших соседей, требующих в среднем O(ln ℓ) операций.
- В простейших случаях метрические алгоритмы имеют крайне бедный набор параметров, что исключает возможность настройки алгоритма по данным.



• Выбрать	оптимальное	значение	k по	критерию	скользящего	контроля	LOO и	построить	график	зависимости	LOO(k).
• Реализовать	алгоритм	k взвешенных	ближайших	соседей	– kwNN.
• Выбрать	оптимальные	значение	k и	q (параметра	весовой	функции	qi)	по	критерию	скользящего	контроля	LOO и	построить	график	зависимости	LOO(k,	h).
• Сравнить	качество	алгоритмов	kNN и	kwNN.
• Построить	карту	классификации	для	рассматриваемых	методов.
• Привести пример,	показывающий	преимущество	метода kwNNнад kNN.

## Метод парзеновского окна
• Реализовать	метод	парзеновского	окна.
• Рассмотреть	несколько	видов	ядер,	для	каждого	из	них	построить	карту	классификации,	оценить	качество,	подобрать.
• оптимальное	значение	ширины	окна	h по	критерию	скользящего	контроля	LOO.
• Сравнить	качество	построенных	алгоритмов	между	собой	и	с	ранее	реализованными методами.

## Метод	потенциальных	функций
• Реализовать	метод	потенциальных	функций	с	фиксированной шириной	окна.
• Сделать	чертеж	демонстрирующий	особенность	метода	(например,	выделить	с	помощью	размера	и	яркости	объекты	с	более	высоким	потенциалом).
• Рассмотреть	несколько	видов	ядер,	для	каждого	из	них	построить	
карту	классификации,	оценить	качество.
• Сравнить	качество	построенного	алгоритма	классификации с	ранее	реализованными методами.

##  Отбор	эталонных	объектов
• Построить	график	отступов	для	объектов	обучения	относительно	произвольного	метрического	классификатора.
• С	помощью	алгоритма	«крутого	склона»	отобрать	шумовые	объекты	–выбросы.
• Реализовать	алгоритм	STOLP для	отбора	опорных	объектов.
• Сравнить	качество	и	скорость	работы	алгоритма	классификации	до	и	после	отбора	объектов	с	помощью	алгоритма	STOLP.


# Тема:	«Байесовские	алгоритмы	классификации»
Данные: Реальные или	модельные.

## Линии	уровня	нормального	распределения
Данные: Центр	и	ковариационная	матрица.
• Отобразить	на	графике	линии	уровня	нормального	распределения	с	указанием	на	них	значения	плотности	распределения.
• Рассмотреть	все	особые	случаи.

## Наивный	нормальный	байесовский	классификатор
• Реализовать	наивный	нормальный байесовский	классификатор.
• Сделать	чертеж, демонстрирующий	работу	данного	метода.	

## Подстановочный	алгоритм	(plug-in)
• Реализовать	подстановочный	байесовский	алгоритм.
• Сделать	чертеж, демонстрирующий	работу	алгоритма.
• Рассмотреть	случаи,	когда	разделяющая	кривая	является:	параболой,	эллипсом	и	гиперболой.	

## Линейный	дискриминант	Фишера	– ЛДФ
• Реализовать	линейный	дискриминант	Фишера.
• Сделать	чертеж,	демонстрирующий	работу	алгоритма.
• Указать	на	преимущества	метода	по	сравнению	с	подстановочным	алгоритмом.	

## Сеть	радиальных	базисных	функций	– RBF сеть
Данные: Реальные или	модельные,	имеющие	в	каждом	классе	два	и	более	сгустка.
• Реализовать	EM-алгоритм	с	последовательным	добавлением	компонент.
• на	основе	EM-алгоритма	обучить	RBF-сеть.
• сделать	чертеж,	демонстрирующий	работу	алгоритма.	


# Тема	«Линейные	алгоритмы	классификации»
## ADALINE.	Правило	Хэбба	(персептрон	Розенблатта)
• Реализовать	метод	стохастического	градиента	для	произвольной	функции	потерь.
• Реализовать	линейный	алгоритм	классификации	ADALINE.
• Построить	линейный	классификатор	с использованием	правила	Хебба	в	методе	стохастического	градиента.
• Сделать	чертежи, демонстрирующие	работу	алгоритмов.
• Сравнить	два	этих	метода.	

## Логистическая	регрессия
• Обучить	алгоритм	логистической	регрессии с	помощью	метода	стохастического	градиента.
• Сделать	чертеж, демонстрирующий	работу	алгоритма.
• В зависимости	от	значения	апостериорной	вероятности	(принадлежности	объекта	к	классу)	раскрасить	чертеж	– чем	большее	значение	вероятности,	тем	темнее	цвет.
• Продемонстрировать	на	одном	графике	работу	реализованных	выше	алгоритмов.	

## Метод	опорных	векторов	– SVM.	ROC-кривая
Язык	программирования: К +	kernlab
• С	помощью	библиотеки	kernlab для	языка	R релализовать	алгоритм	SVM.
• Реализовать алгоритм	построения	ROC-кривой и	вычисления	AUC.
• Продемонстрировать	работу	метода	для	случаев:	
o линейно	разделимой	выборки;
o линенейно	неразделимой	выборки с	малым	числом	объектов,	«мешающих» линейной	отделимости;
o линенейно	неразделимой	выборки с	средним числом	объектов,	«мешающих» линейной	отделимости.
